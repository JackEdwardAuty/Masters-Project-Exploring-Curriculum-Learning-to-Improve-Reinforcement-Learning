# Curriculum Learning for Reinforcement Learning - Complete Codebase

A comprehensive implementation of **Curriculum Learning** applied to **Reinforcement Learning**, demonstrating how training agents on progressively harder tasks improves convergence speed and final performance compared to traditional RL approaches.

## Project Overview

This project explores curriculum learning techniques using:
- **Environment**: Modified OpenAI Multi-Agent Emergence Environment (simplified to single-agent)
- **Algorithm**: Proximal Policy Optimization (PPO) via Stable Baselines 3
- **Physics Engine**: MuJoCo
- **Progression Functions**: Linear, Exponential, and Friction-Based (Adaptive)

### Key Results

From the original research:
- **Friction-Based Progression**: ~80% success rate
- **Exponential Progression**: ~70% success rate  
- **No Curriculum (Baseline)**: ~18% success rate

This demonstrates the **massive improvement** curriculum learning provides!

## Project Structure

```
curriculum-learning/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.yaml                    # Training configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ curriculum_env.py          # Main environment with curriculum support
â”‚   â”‚   â””â”€â”€ base_env.py                # Base environment class (MuJoCo-based)
â”‚   â”œâ”€â”€ progression/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ progression_functions.py   # Linear, Exponential, Friction-Based
â”‚   â”‚   â””â”€â”€ mapping_functions.py       # Complexity to task mapping
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py                 # Training loop orchestrator
â”‚   â”‚   â”œâ”€â”€ callbacks.py               # Custom callbacks for monitoring
â”‚   â”‚   â””â”€â”€ parallel_training.py       # Parallel environment execution
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py                  # Experiment logging
â”‚   â”‚   â”œâ”€â”€ plotting.py                # Visualization utilities
â”‚   â”‚   â””â”€â”€ helpers.py                 # Helper functions
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ ppo_agent.py               # PPO agent wrapper
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                       # Main training script
â”‚   â”œâ”€â”€ evaluate.py                    # Evaluation script
â”‚   â””â”€â”€ plot_results.py                # Plot training results
â””â”€â”€ experiments/
    â”œâ”€â”€ results/                       # Training results and logs
    â””â”€â”€ checkpoints/                   # Model checkpoints
```

## Installation

1. **Clone the repository**:
```bash
git clone <your-repo>
cd curriculum-learning
```

2. **Create a virtual environment**:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Note about MuJoCo**: MuJoCo is now free! However, you still need to set up your MuJoCo key:
   - Get it from: https://www.deepmind.com/documents/314/mujoco_license.txt
   - Store in `~/.mujoco/mjkey.txt`

## Usage

### Basic Training with Curriculum Learning

```bash
# Train with Friction-Based (Adaptive) progression
python scripts/train.py --progression friction --num_processes 4 --total_steps 1000000

# Train with Exponential progression
python scripts/train.py --progression exponential --slope 0.73 --num_processes 4

# Train without curriculum (baseline)
python scripts/train.py --progression none --num_processes 4
```

### Configuration

Edit `config/config.yaml` to customize:
- Training hyperparameters (learning rate, batch size, epochs)
- Environment parameters (floor size, target speed)
- Progression function settings
- Parallel execution parameters

### Evaluation

```bash
# Evaluate trained model
python scripts/evaluate.py --model experiments/checkpoints/best_model.zip --episodes 100

# Plot training curves
python scripts/plot_results.py --log-dir experiments/results/
```

## Key Components

### 1. Curriculum Environment (`src/environment/curriculum_env.py`)

Handles:
- Single-agent seeker tracking a moving target
- Complexity parameters: floor size, target speed
- Reward: +1 if target found, 0 otherwise
- Episode termination: target found or max steps (100)

### 2. Progression Functions (`src/progression/progression_functions.py`)

**Linear Progression**:
```
Î _l(t, t_e) = max(t/t_e, 1)
```

**Exponential Progression**:
```
Î _e(t, t_e, s) = 1 - max((Î± - Î²) / (1 - Î²), 0)
where Î± = exp(-t / (t_e * s)), Î² = exp(-1/s)
```

**Friction-Based (Adaptive)**:
```
Î _f(t, c_{t-1}) = 1 - Uniform(s_t, s_min)
Adapts based on agent performance
```

### 3. Mapping Function (`src/progression/mapping_functions.py`)

Maps complexity factor (0 to 1) to task parameters:
```
M_t(s, f) = Î¦(c_t) = (0.89 * c_t, 22 * c_t + 2)
Target Speed: [0, 0.89]
Floor Size: [2, 24]
```

### 4. Training Pipeline (`src/training/trainer.py`)

Orchestrates:
- Parallel environment execution
- PPO training with multiple processes
- Progression function updates
- Logging and checkpointing

## Experimental Results

### Performance Comparison (from original project)

| Method | Final Success Rate | Convergence Speed | Notes |
|--------|------------------|-------------------|-------|
| Friction-Based | ~80% | Fast | Adaptive to agent performance |
| Exponential | ~70% | Medium | Pre-determined curriculum |
| No Curriculum | ~18% | Very Slow | Baseline - maximum difficulty |

### Key Observations

1. **Curriculum Learning is Effective**: 4-5x improvement over baseline
2. **Adaptive > Fixed**: Friction-Based outperforms Exponential by ~10%
3. **Faster Convergence**: Curriculum methods reach peak performance in ~half the time
4. **Transfer Learning**: Knowledge transfers across difficulty levels

## Advanced Usage

### Custom Progression Function

```python
from src.progression import ProgressionFunction

class CustomProgression(ProgressionFunction):
    def __call__(self, timestep, previous_complexity):
        # Your logic here
        return new_complexity
```

### Multi-Agent Extension

The codebase is designed for easy extension to multiple agents. Uncomment multi-agent sections in `curriculum_env.py`.

### Custom Reward Functions

Modify the `get_reward()` method in `CurriculumEnvironment` to implement:
- Distance-based rewards
- Time penalties
- Multi-objective rewards
- Sparse vs. dense rewards

## Troubleshooting

### MuJoCo Installation Issues
```bash
# Try alternative installation
pip install mujoco dm-control
```

### Out of Memory with Parallel Training
Reduce `num_processes` in config or command line:
```bash
python scripts/train.py --num_processes 2
```

### Training Divergence
- Reduce learning rate: `--learning_rate 0.0001`
- Increase batch size: `--batch_size 2048`
- Use shorter progression: `--progression_steps 100000`

## Project Status

âœ… Core implementation complete
âœ… Friction-based progression working
âœ… Parallel training support
âœ… Results matching original paper
âš ï¸ Multi-agent version in development
âš ï¸ Advanced obstacle support planned

## References

1. **Curriculum Learning Paper**: Bassich et al., "Curriculum learning with a progression function" (2020) [arXiv:2008.00511](https://arxiv.org/abs/2008.00511)

2. **Multi-Agent Emergence**: Baker et al., "Emergent tool use from multi-agent autocurricula" (2020)

3. **PPO Algorithm**: Schulman et al., "Proximal Policy Optimization Algorithms" (2017)

## License

This project is for educational and research purposes. Uses open-source dependencies.

## Contributing

Feel free to:
- Open issues for bugs or questions
- Submit pull requests for enhancements
- Share results and findings
- Suggest improvements

## Contact

For questions about the implementation or curriculum learning concepts, reach out!

---

**Happy Training! ğŸ“ğŸš€**

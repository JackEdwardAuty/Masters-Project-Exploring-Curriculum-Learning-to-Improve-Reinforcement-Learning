
@inproceedings{inproceedings01,
	address = {Montreal, Quebec, Canada},
	series = {{ICML} '09},
	title = {Curriculum learning},
	isbn = {9781605585161},
	doi = {10.1145/1553374.1553380},
	abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
	month = jun,
	year = {2009},
	pages = {41--48},
}

@article{curriculum-matteo,
	title = {Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},
	volume = {21},
	pages = {1--50},
	number = {181},
	journaltitle = {Journal of Machine Learning Research},
	author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
	date = {2020},
}

@inproceedings{
multi-agent-tool,
title={Emergent Tool Use From Multi-Agent Autocurricula},
author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkxpxJBKwS}
}

@misc{misc01,
      title={Curriculum Learning with a Progression Function}, 
      author={Andrea Bassich and Francesco Foglino and Matteo Leonetti and Daniel Kudenko},
      year={2020},
      eprint={2008.00511},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@book{book01,
	location = {Cambridge, Massachusetts},
	edition = {Second edition},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	pagetotal = {526},
	publisher = {The {MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {2018},
	keywords = {Reinforcement learning},
}


@book{bo01,
      title={Reinforcement Learning and Markov Decision Processes}, 
      author={Martijn van Otterlo and Marco Wiering},
      year={2012},
      eprint={2008.00511},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{misc02,
    title={OpenAI Gym},
    author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
    year={2016},
    eprint={1606.01540},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{mojoco,
  author={E. {Todorov} and T. {Erez} and Y. {Tassa}},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}}
  
@misc{baker2019emergent,
    title={Emergent Tool Use From Multi-Agent Autocurricula},
    author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
    year={2019},
    eprint={1909.07528},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{github,
    title={multi-agent-emergence-environments},
    author={Baker, Bowen and Markov, Todor},
    year={2020},
    url={https://github.com/openai/multi-agent-emergence-environments},
    publisher={OpenAI}
}

@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  volume={48},
  number={1},
  pages={71--99},
  year={1993},
  publisher={Elsevier}
}

@article{sanger1994neural,
  title={Neural network learning control of robot manipulators using gradually increasing task difficulty},
  author={Sanger, Terence D},
  journal={IEEE transactions on Robotics and Automation},
  volume={10},
  number={3},
  pages={323--333},
  year={1994},
  publisher={IEEE}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@inproceedings{lazaric2008transfer,
  title={Transfer of samples in batch reinforcement learning},
  author={Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={544--551},
  year={2008}
}

@article{taylor2007transfer,
  title={Transfer Learning via Inter-Task Mappings for Temporal Difference Learning.},
  author={Taylor, Matthew E and Stone, Peter and Liu, Yaxin},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={9},
  year={2007}
}

@book{book02,
	location = {Berlin, Heidelberg},
	edition = {First edition},
	title = {Reinforcement Learning},
	isbn = {978-3-642-27645-3},
	series = {Adaptation, Learning, and Optimization},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning encompasses both a science of adaptive behavior of rational beings in uncertain environments and a computational methodology for finding optimal behaviors for challenging problems in control, optimization and adaptive behavior of intelligent agents. As a field, reinforcement learning has progressed tremendously in the past decade.

    The main goal of this book is to present an up-to-date series of survey articles on the main contemporary sub-fields of reinforcement learning. This includes surveys on partially observable environments, hierarchical task decompositions, relational knowledge representation and predictive state representations. Furthermore, topics such as transfer, evolutionary methods and continuous spaces in reinforcement learning are surveyed. In addition, several chapters review reinforcement learning methods in robotics, in games, and in computational neuroscience. In total seventeen different subfields are presented by mostly young experts in those areas, and together they truly represent a state-of-the-art of current reinforcement learning research.

    Marco Wiering works at the artificial intelligence department of the University of Groningen in the Netherlands. He has published extensively on various reinforcement learning topics. Martijn van Otterlo works in the cognitive artificial intelligence group at the Radboud University Nijmegen in The Netherlands. He has mainly focused on expressive knowledge representation in reinforcement learning settings."--},
	url = {https://doi.org/10.1007/978-3-642-27645-3},
	pagetotal = {638},
	publisher = {Springer},
	author = {Martijn van Otterlo and Marco Wiering},
	date = {2012},
	keywords = {Artificial Intelligence, Computational Intelligence, Decision-Theoretic Planning, Dynamic Programming, Machine Learning, Markov Decision Processes, Optimal Control, Reinforcement Learning, Utility-Based Learning}
}

@misc{2018RLGuide,
      title={What is reinforcement learning? The complete guide}, 
      author={Błażej Osiński and Konrad Budek},
      year={2018},
      url = {https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/},
      publisher={deepsense.ai}
}


@misc{expertmldef,
      title={What is reinforcement learning? The complete guide}, 
      author={Expert.ai Team},
      year={2020},
      url = {https://www.expert.ai/blog/machine-learning-definition/},
      publisher={Expert.ai}
}

@misc{whatisml,
      title={What is machine learning and types of machine learning --Part-1}, 
      author={Chinmay Das},
      year={2017},
      url = {https://towardsdatascience.com/what-is-machine-learning-and-types-of-machine-learning-andrews-machine-learning-part-1-9cd9755bc647},
      publisher={towards data science}
}

@ARTICLE{5392560,
  author={Samuel, A. L.},
  journal={IBM Journal of Research and Development}, 
  title={Some Studies in Machine Learning Using the Game of Checkers}, 
  year={1959},
  volume={3},
  number={3},
  pages={210-229},
  doi={10.1147/rd.33.0210}}
 
@misc{goodfellow2016deep,
  title={Deep Learning (Adaptive Computation and Machine Learning series)},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  page={526},
  publisher={e MIT Press, Cambridge, England}
}

@misc{yosinski2014transferable,
      title={How transferable are features in deep neural networks?}, 
      author={Jason Yosinski and Jeff Clune and Yoshua Bengio and Hod Lipson},
      year={2014},
      eprint={1411.1792},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@incollection{lazaric2012transfer,
  title={Transfer in reinforcement learning: a framework and a survey},
  author={Lazaric, Alessandro},
  booktitle={Reinforcement Learning},
  pages={143--173},
  year={2012},
  publisher={Springer}
}

@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey.},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={7},
  pages={1633-1685},
  year={2009}
}

@article{samuel_1959,
  title={Some Studies in Machine Learning Using the Game of Checkers},
  volume={3}, DOI={10.1147/rd.33.0210},
  number={3},
  journal={IBM Journal of Research and Development},
  author={Samuel, A. L.},
  year={1959},
  pages={210–229}
}

@misc{deepmind,
  title={The Google DeepMind Challenge Match, March 2016},
  url={https://deepmind.com/alphago-korea},
  journal={Deepmind}
}

@misc{silver_hassabis_2016,
  title={AlphaGo: Mastering the ancient game of Go with Machine Learning}, url={https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html},
  journal={Google AI Blog},
  author={Silver, David and Hassabis, Demis},
  year={2016},
  month={Jan}
}

@article{müller_2002,
  title={Computer Go},
  volume={134},
  DOI={10.1016/s0004-3702(01)00121-7},
  number={1-2},
  journal={Artificial Intelligence},
  author={Müller, Martin},
  year={2002}, month={Jan},
  pages={145–179}
}

@article{general_2018,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  volume={362},
  DOI={10.1126/science.aar6404}, number={6419},
  journal={Science},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and et al.},
  year={2018},
  month={Dec},
  pages={1140–1144}
}

@article{openai_learning_2018,
  title = {Learning {Dexterous} {In}-{Hand} {Manipulation}},
  url = {http://arxiv.org/abs/1808.00177},
  journal = {CoRR},
  author = {{OpenAI} and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Józefowicz, Rafał and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  year = {2018},
}

@misc{rl_hierarchy_source,
  title={Part 2: Kinds of RL Algorithms¶},
  url={https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#citations-below},
  journal={Part 2: Kinds of RL Algorithms - Spinning Up documentation}
}
%2
@misc{ppo_spin,
    title={OpenAI Spinning Up, Proximal Policy Optimization},
    url={https://spinningup.openai.com/en/latest/algorithms/ppo.html}
}
%3
@misc{vpg_spin,
    title={OpenAI Spining UP, Vanilla Policy Gradient},
    url={https://spinningup.openai.com/en/latest/algorithms/vpg.html}
}

%4
@misc{ppo-zhihu,
    title={Proximal Policy Optimization (PPO), Zhihu},
    url={https://zhuanlan.zhihu.com/p/62654423}
}



%fig1
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

%fig2
@misc{bellman-equation,
    title={Barnabás Póczos, Introduction to Machine Learning},
    url={https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture26-ri.pdf },
}

%fig3
@misc{bellman-optimality,
    title={Blackburn, Reinforcement Learning: Bellman Equation and Optimality (Part2)},
    url={https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3},
}


